Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-0-200-2200
Total Params. :  4,120,825
Total Trainable Params. :  4,120,825
{'loss': 11750.315, 'grad_norm': 1216.1905517578125, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 2002.3914794921875, 'eval_runtime': 3.8745, 'eval_samples_per_second': 2206.977, 'eval_steps_per_second': 34.585, 'epoch': 0.37}
{'loss': 352.0288, 'grad_norm': 1.134652853012085, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 1.8334074020385742, 'eval_runtime': 4.5481, 'eval_samples_per_second': 1880.124, 'eval_steps_per_second': 29.463, 'epoch': 0.75}
{'loss': 1.0831, 'grad_norm': 0.1855519413948059, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 0.9084151983261108, 'eval_runtime': 4.0023, 'eval_samples_per_second': 2136.527, 'eval_steps_per_second': 33.481, 'epoch': 1.12}
{'loss': 0.9627, 'grad_norm': 0.19412700831890106, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 0.8971402645111084, 'eval_runtime': 4.4861, 'eval_samples_per_second': 1906.106, 'eval_steps_per_second': 29.87, 'epoch': 1.5}
{'loss': 1.0369, 'grad_norm': 0.17583078145980835, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.8946594595909119, 'eval_runtime': 4.2595, 'eval_samples_per_second': 2007.524, 'eval_steps_per_second': 31.459, 'epoch': 1.87}
{'loss': 0.9876, 'grad_norm': 0.19966059923171997, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.8878140449523926, 'eval_runtime': 4.1483, 'eval_samples_per_second': 2061.339, 'eval_steps_per_second': 32.303, 'epoch': 2.25}
{'loss': 0.9992, 'grad_norm': 0.24093139171600342, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.8683937191963196, 'eval_runtime': 4.518, 'eval_samples_per_second': 1892.632, 'eval_steps_per_second': 29.659, 'epoch': 2.62}
{'loss': 0.9686, 'grad_norm': 0.27393683791160583, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.8140413761138916, 'eval_runtime': 4.3901, 'eval_samples_per_second': 1947.794, 'eval_steps_per_second': 30.523, 'epoch': 2.99}
{'loss': 0.8382, 'grad_norm': 0.29516395926475525, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.7509061694145203, 'eval_runtime': 4.5584, 'eval_samples_per_second': 1875.863, 'eval_steps_per_second': 29.396, 'epoch': 3.37}
{'loss': 0.8403, 'grad_norm': 0.29663676023483276, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.6652719378471375, 'eval_runtime': 4.4232, 'eval_samples_per_second': 1933.205, 'eval_steps_per_second': 30.295, 'epoch': 3.74}
{'loss': 0.7479, 'grad_norm': 0.27154552936553955, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.6066778898239136, 'eval_runtime': 4.0449, 'eval_samples_per_second': 2114.046, 'eval_steps_per_second': 33.129, 'epoch': 4.12}
{'loss': 0.6688, 'grad_norm': 0.21082811057567596, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.5692821145057678, 'eval_runtime': 4.5799, 'eval_samples_per_second': 1867.063, 'eval_steps_per_second': 29.258, 'epoch': 4.49}
{'loss': 0.6751, 'grad_norm': 0.36419209837913513, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.5478829145431519, 'eval_runtime': 4.6146, 'eval_samples_per_second': 1853.047, 'eval_steps_per_second': 29.039, 'epoch': 4.86}
{'loss': 0.6731, 'grad_norm': 0.3104815185070038, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.5314865112304688, 'eval_runtime': 4.3876, 'eval_samples_per_second': 1948.919, 'eval_steps_per_second': 30.541, 'epoch': 5.24}
{'loss': 0.5849, 'grad_norm': 0.24608369171619415, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.5171782374382019, 'eval_runtime': 4.4284, 'eval_samples_per_second': 1930.944, 'eval_steps_per_second': 30.259, 'epoch': 5.61}
{'loss': 0.6889, 'grad_norm': 0.31240078806877136, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.5056736469268799, 'eval_runtime': 4.1737, 'eval_samples_per_second': 2048.766, 'eval_steps_per_second': 32.106, 'epoch': 5.99}
{'loss': 0.5701, 'grad_norm': 0.2899007499217987, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.49155497550964355, 'eval_runtime': 4.5889, 'eval_samples_per_second': 1863.391, 'eval_steps_per_second': 29.201, 'epoch': 6.36}
{'loss': 0.6049, 'grad_norm': 0.5962454080581665, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.48131197690963745, 'eval_runtime': 4.5464, 'eval_samples_per_second': 1880.832, 'eval_steps_per_second': 29.474, 'epoch': 6.74}
{'loss': 0.6446, 'grad_norm': 0.29811131954193115, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.47191283106803894, 'eval_runtime': 4.333, 'eval_samples_per_second': 1973.46, 'eval_steps_per_second': 30.925, 'epoch': 7.11}
{'loss': 0.5826, 'grad_norm': 0.3818349540233612, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.4632832109928131, 'eval_runtime': 4.3784, 'eval_samples_per_second': 1953.006, 'eval_steps_per_second': 30.605, 'epoch': 7.48}
{'loss': 0.5502, 'grad_norm': 0.328653484582901, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.4553700387477875, 'eval_runtime': 4.2565, 'eval_samples_per_second': 2008.94, 'eval_steps_per_second': 31.481, 'epoch': 7.86}
{'loss': 0.545, 'grad_norm': 0.3286117613315582, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.4499692916870117, 'eval_runtime': 4.2848, 'eval_samples_per_second': 1995.653, 'eval_steps_per_second': 31.273, 'epoch': 8.23}
{'loss': 0.515, 'grad_norm': 0.36753329634666443, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.4446834325790405, 'eval_runtime': 4.5755, 'eval_samples_per_second': 1868.877, 'eval_steps_per_second': 29.287, 'epoch': 8.61}
{'loss': 0.6079, 'grad_norm': 0.34891819953918457, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.4418265223503113, 'eval_runtime': 4.4267, 'eval_samples_per_second': 1931.691, 'eval_steps_per_second': 30.271, 'epoch': 8.98}
{'loss': 0.6019, 'grad_norm': 0.4041581451892853, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.43930667638778687, 'eval_runtime': 4.2849, 'eval_samples_per_second': 1995.605, 'eval_steps_per_second': 31.272, 'epoch': 9.35}
{'loss': 0.5199, 'grad_norm': 0.31571659445762634, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.43775978684425354, 'eval_runtime': 4.1237, 'eval_samples_per_second': 2073.64, 'eval_steps_per_second': 32.495, 'epoch': 9.73}
{'train_runtime': 978.1004, 'train_samples_per_second': 699.356, 'train_steps_per_second': 10.929, 'train_loss': 453.51598017436527, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-1-200-2200
Total Params. :  3,866,441
Total Trainable Params. :  3,866,441
{'loss': 12089.4163, 'grad_norm': 3087.94677734375, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 1512.65234375, 'eval_runtime': 4.0483, 'eval_samples_per_second': 2112.267, 'eval_steps_per_second': 33.101, 'epoch': 0.37}
{'loss': 201.5822, 'grad_norm': 1.958022117614746, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 1.6766574382781982, 'eval_runtime': 3.9338, 'eval_samples_per_second': 2173.706, 'eval_steps_per_second': 34.063, 'epoch': 0.75}
{'loss': 2.0149, 'grad_norm': 0.8980585336685181, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 1.0615297555923462, 'eval_runtime': 4.3038, 'eval_samples_per_second': 1986.87, 'eval_steps_per_second': 31.136, 'epoch': 1.12}
{'loss': 1.5407, 'grad_norm': 1.228735089302063, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 1.0033197402954102, 'eval_runtime': 3.9566, 'eval_samples_per_second': 2161.205, 'eval_steps_per_second': 33.868, 'epoch': 1.5}
{'loss': 1.4587, 'grad_norm': 0.7355640530586243, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.9370265007019043, 'eval_runtime': 4.3548, 'eval_samples_per_second': 1963.598, 'eval_steps_per_second': 30.771, 'epoch': 1.87}
{'loss': 1.3302, 'grad_norm': 0.7098230719566345, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.9322261810302734, 'eval_runtime': 3.8966, 'eval_samples_per_second': 2194.475, 'eval_steps_per_second': 34.389, 'epoch': 2.25}
{'loss': 1.2637, 'grad_norm': 1.2004646062850952, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.8542190194129944, 'eval_runtime': 4.4415, 'eval_samples_per_second': 1925.268, 'eval_steps_per_second': 30.17, 'epoch': 2.62}
{'loss': 1.1732, 'grad_norm': 1.219006896018982, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.7644650936126709, 'eval_runtime': 4.6989, 'eval_samples_per_second': 1819.794, 'eval_steps_per_second': 28.517, 'epoch': 2.99}
{'loss': 1.0173, 'grad_norm': 0.7304101586341858, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.6874266266822815, 'eval_runtime': 4.7016, 'eval_samples_per_second': 1818.75, 'eval_steps_per_second': 28.501, 'epoch': 3.37}
{'loss': 1.0207, 'grad_norm': 1.155593752861023, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.6344444155693054, 'eval_runtime': 4.7053, 'eval_samples_per_second': 1817.322, 'eval_steps_per_second': 28.479, 'epoch': 3.74}
{'loss': 0.9309, 'grad_norm': 0.7334393262863159, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.5961111783981323, 'eval_runtime': 4.3157, 'eval_samples_per_second': 1981.39, 'eval_steps_per_second': 31.05, 'epoch': 4.12}
{'loss': 0.8697, 'grad_norm': 0.6817572712898254, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.5934155583381653, 'eval_runtime': 4.4235, 'eval_samples_per_second': 1933.089, 'eval_steps_per_second': 30.293, 'epoch': 4.49}
{'loss': 0.8851, 'grad_norm': 0.6984915733337402, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.5840475559234619, 'eval_runtime': 4.239, 'eval_samples_per_second': 2017.226, 'eval_steps_per_second': 31.611, 'epoch': 4.86}
{'loss': 0.8751, 'grad_norm': 1.132412314414978, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.5669531226158142, 'eval_runtime': 4.2237, 'eval_samples_per_second': 2024.518, 'eval_steps_per_second': 31.726, 'epoch': 5.24}
{'loss': 0.7774, 'grad_norm': 0.9525781273841858, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.5596762895584106, 'eval_runtime': 4.4991, 'eval_samples_per_second': 1900.604, 'eval_steps_per_second': 29.784, 'epoch': 5.61}
{'loss': 0.8854, 'grad_norm': 0.6853470206260681, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.5469309687614441, 'eval_runtime': 4.1607, 'eval_samples_per_second': 2055.184, 'eval_steps_per_second': 32.206, 'epoch': 5.99}
{'loss': 0.7591, 'grad_norm': 0.6474655866622925, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.5378650426864624, 'eval_runtime': 4.1581, 'eval_samples_per_second': 2056.465, 'eval_steps_per_second': 32.226, 'epoch': 6.36}
{'loss': 0.7997, 'grad_norm': 1.0941290855407715, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.5286345481872559, 'eval_runtime': 4.2865, 'eval_samples_per_second': 1994.851, 'eval_steps_per_second': 31.261, 'epoch': 6.74}
{'loss': 0.8484, 'grad_norm': 1.8239840269088745, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.5220422744750977, 'eval_runtime': 4.1357, 'eval_samples_per_second': 2067.624, 'eval_steps_per_second': 32.401, 'epoch': 7.11}
{'loss': 0.7791, 'grad_norm': 0.7295801639556885, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.527933657169342, 'eval_runtime': 4.0628, 'eval_samples_per_second': 2104.72, 'eval_steps_per_second': 32.982, 'epoch': 7.48}
{'loss': 0.7483, 'grad_norm': 0.9387668371200562, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.5268752574920654, 'eval_runtime': 4.4127, 'eval_samples_per_second': 1937.798, 'eval_steps_per_second': 30.367, 'epoch': 7.86}
{'loss': 0.7454, 'grad_norm': 0.7046492695808411, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.5178770422935486, 'eval_runtime': 4.1444, 'eval_samples_per_second': 2063.267, 'eval_steps_per_second': 32.333, 'epoch': 8.23}
{'loss': 0.7126, 'grad_norm': 1.4233267307281494, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.5232471823692322, 'eval_runtime': 3.9766, 'eval_samples_per_second': 2150.319, 'eval_steps_per_second': 33.697, 'epoch': 8.61}
{'loss': 0.8242, 'grad_norm': 1.1110684871673584, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.5168896317481995, 'eval_runtime': 4.5216, 'eval_samples_per_second': 1891.154, 'eval_steps_per_second': 29.636, 'epoch': 8.98}
{'loss': 0.812, 'grad_norm': 0.8579075336456299, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.5160310864448547, 'eval_runtime': 4.6069, 'eval_samples_per_second': 1856.119, 'eval_steps_per_second': 29.087, 'epoch': 9.35}
{'loss': 0.7221, 'grad_norm': 0.9783796072006226, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.5158868432044983, 'eval_runtime': 4.1765, 'eval_samples_per_second': 2047.385, 'eval_steps_per_second': 32.084, 'epoch': 9.73}
{'train_runtime': 976.2334, 'train_samples_per_second': 700.693, 'train_steps_per_second': 10.95, 'train_loss': 460.81620373917696, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-3-200-2200
Total Params. :  12,752,841
Total Trainable Params. :  12,752,841
{'loss': 7743.9244, 'grad_norm': 1.9445656538009644, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 1.0099029541015625, 'eval_runtime': 4.4676, 'eval_samples_per_second': 1913.989, 'eval_steps_per_second': 29.994, 'epoch': 0.37}
{'loss': 1.4098, 'grad_norm': 1.4979139566421509, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 2.0170228481292725, 'eval_runtime': 4.5622, 'eval_samples_per_second': 1874.321, 'eval_steps_per_second': 29.372, 'epoch': 0.75}
{'loss': 1.0892, 'grad_norm': 3.014294385910034, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 8.385895729064941, 'eval_runtime': 3.8815, 'eval_samples_per_second': 2203.008, 'eval_steps_per_second': 34.523, 'epoch': 1.12}
{'loss': 1.0363, 'grad_norm': 1.9928700923919678, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 11.363754272460938, 'eval_runtime': 4.4623, 'eval_samples_per_second': 1916.282, 'eval_steps_per_second': 30.029, 'epoch': 1.5}
{'loss': 1.0107, 'grad_norm': 2.099541425704956, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 8.697815895080566, 'eval_runtime': 4.1369, 'eval_samples_per_second': 2067.022, 'eval_steps_per_second': 32.392, 'epoch': 1.87}
{'loss': 0.7904, 'grad_norm': 3.4232406616210938, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 8.960692405700684, 'eval_runtime': 4.4866, 'eval_samples_per_second': 1905.879, 'eval_steps_per_second': 29.866, 'epoch': 2.25}
{'loss': 0.746, 'grad_norm': 1.9112271070480347, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 8.135164260864258, 'eval_runtime': 3.9753, 'eval_samples_per_second': 2151.059, 'eval_steps_per_second': 33.709, 'epoch': 2.62}
{'loss': 0.7218, 'grad_norm': 2.797457218170166, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 7.9169111251831055, 'eval_runtime': 4.3322, 'eval_samples_per_second': 1973.836, 'eval_steps_per_second': 30.931, 'epoch': 2.99}
{'loss': 0.63, 'grad_norm': 2.7825570106506348, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 9.065412521362305, 'eval_runtime': 4.0679, 'eval_samples_per_second': 2102.043, 'eval_steps_per_second': 32.94, 'epoch': 3.37}
{'loss': 0.6903, 'grad_norm': 2.213975429534912, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 8.290973663330078, 'eval_runtime': 4.3433, 'eval_samples_per_second': 1968.799, 'eval_steps_per_second': 30.852, 'epoch': 3.74}
{'loss': 0.6469, 'grad_norm': 1.7872848510742188, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 7.718775749206543, 'eval_runtime': 4.1759, 'eval_samples_per_second': 2047.709, 'eval_steps_per_second': 32.089, 'epoch': 4.12}
{'loss': 0.602, 'grad_norm': 2.694385290145874, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 9.070048332214355, 'eval_runtime': 4.4517, 'eval_samples_per_second': 1920.841, 'eval_steps_per_second': 30.101, 'epoch': 4.49}
{'loss': 0.6281, 'grad_norm': 3.9632585048675537, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 6.851187705993652, 'eval_runtime': 4.5434, 'eval_samples_per_second': 1882.086, 'eval_steps_per_second': 29.494, 'epoch': 4.86}
{'loss': 0.6201, 'grad_norm': 2.511793375015259, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 6.8742570877075195, 'eval_runtime': 4.2182, 'eval_samples_per_second': 2027.183, 'eval_steps_per_second': 31.767, 'epoch': 5.24}
{'loss': 0.544, 'grad_norm': 3.2270212173461914, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 8.269601821899414, 'eval_runtime': 4.1193, 'eval_samples_per_second': 2075.822, 'eval_steps_per_second': 32.53, 'epoch': 5.61}
{'loss': 0.628, 'grad_norm': 1.7890785932540894, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 7.534902572631836, 'eval_runtime': 3.8988, 'eval_samples_per_second': 2193.242, 'eval_steps_per_second': 34.37, 'epoch': 5.99}
{'loss': 0.5268, 'grad_norm': 1.5727473497390747, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 7.561530590057373, 'eval_runtime': 4.4915, 'eval_samples_per_second': 1903.8, 'eval_steps_per_second': 29.834, 'epoch': 6.36}
{'loss': 0.5694, 'grad_norm': 1.7350419759750366, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 7.603351593017578, 'eval_runtime': 4.7465, 'eval_samples_per_second': 1801.544, 'eval_steps_per_second': 28.231, 'epoch': 6.74}
{'loss': 0.594, 'grad_norm': 2.2378859519958496, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 7.327828407287598, 'eval_runtime': 4.2373, 'eval_samples_per_second': 2018.037, 'eval_steps_per_second': 31.624, 'epoch': 7.11}
{'loss': 0.5397, 'grad_norm': 1.8266408443450928, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 6.838478088378906, 'eval_runtime': 4.2004, 'eval_samples_per_second': 2035.779, 'eval_steps_per_second': 31.902, 'epoch': 7.48}
{'loss': 0.5154, 'grad_norm': 3.625924587249756, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 8.14143180847168, 'eval_runtime': 4.1863, 'eval_samples_per_second': 2042.609, 'eval_steps_per_second': 32.009, 'epoch': 7.86}
{'loss': 0.511, 'grad_norm': 1.4509782791137695, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 6.6096014976501465, 'eval_runtime': 4.2888, 'eval_samples_per_second': 1993.81, 'eval_steps_per_second': 31.244, 'epoch': 8.23}
{'loss': 0.4842, 'grad_norm': 1.5664293766021729, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 6.779433250427246, 'eval_runtime': 4.1065, 'eval_samples_per_second': 2082.31, 'eval_steps_per_second': 32.631, 'epoch': 8.61}
{'loss': 0.5731, 'grad_norm': 1.5747205018997192, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 6.677966117858887, 'eval_runtime': 3.9539, 'eval_samples_per_second': 2162.69, 'eval_steps_per_second': 33.891, 'epoch': 8.98}
{'loss': 0.5707, 'grad_norm': 1.4617769718170166, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 6.711269855499268, 'eval_runtime': 4.0913, 'eval_samples_per_second': 2090.041, 'eval_steps_per_second': 32.752, 'epoch': 9.35}
{'loss': 0.4876, 'grad_norm': 1.9610577821731567, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 6.899311065673828, 'eval_runtime': 4.4953, 'eval_samples_per_second': 1902.194, 'eval_steps_per_second': 29.809, 'epoch': 9.73}
{'train_runtime': 990.113, 'train_samples_per_second': 690.871, 'train_steps_per_second': 10.797, 'train_loss': 290.41879015959125, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***


Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-5-200-2200
Total Params. :  22,437,241
Total Trainable Params. :  22,437,241
{'loss': 6624.2163, 'grad_norm': 8.13204288482666, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 1.0843427181243896, 'eval_runtime': 6.7174, 'eval_samples_per_second': 1272.954, 'eval_steps_per_second': 19.948, 'epoch': 0.37}
{'loss': 1.5647, 'grad_norm': 8.18396282196045, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 2.733478546142578, 'eval_runtime': 6.4977, 'eval_samples_per_second': 1316.012, 'eval_steps_per_second': 20.623, 'epoch': 0.75}
{'loss': 1.1353, 'grad_norm': 4.407040119171143, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 1.0855249166488647, 'eval_runtime': 6.9194, 'eval_samples_per_second': 1235.802, 'eval_steps_per_second': 19.366, 'epoch': 1.12}
{'loss': 1.0406, 'grad_norm': 1.767602562904358, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 0.969342052936554, 'eval_runtime': 6.6677, 'eval_samples_per_second': 1282.453, 'eval_steps_per_second': 20.097, 'epoch': 1.5}
{'loss': 1.072, 'grad_norm': 2.4942703247070312, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.8891006112098694, 'eval_runtime': 6.668, 'eval_samples_per_second': 1282.386, 'eval_steps_per_second': 20.096, 'epoch': 1.87}
{'loss': 1.0094, 'grad_norm': 3.3983473777770996, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.8790553212165833, 'eval_runtime': 6.8176, 'eval_samples_per_second': 1254.263, 'eval_steps_per_second': 19.655, 'epoch': 2.25}
{'loss': 1.0169, 'grad_norm': 0.7496424317359924, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.8854766488075256, 'eval_runtime': 7.284, 'eval_samples_per_second': 1173.936, 'eval_steps_per_second': 18.396, 'epoch': 2.62}
{'loss': 1.0159, 'grad_norm': 1.2038938999176025, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.8758974671363831, 'eval_runtime': 7.0787, 'eval_samples_per_second': 1207.994, 'eval_steps_per_second': 18.93, 'epoch': 2.99}
{'loss': 0.9363, 'grad_norm': 0.5863431692123413, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.8737401962280273, 'eval_runtime': 7.2052, 'eval_samples_per_second': 1186.774, 'eval_steps_per_second': 18.598, 'epoch': 3.37}
{'loss': 1.0117, 'grad_norm': 0.9591744542121887, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.8677200078964233, 'eval_runtime': 7.0465, 'eval_samples_per_second': 1213.518, 'eval_steps_per_second': 19.017, 'epoch': 3.74}
{'loss': 0.9146, 'grad_norm': 1.4809837341308594, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.7241441607475281, 'eval_runtime': 7.1872, 'eval_samples_per_second': 1189.759, 'eval_steps_per_second': 18.644, 'epoch': 4.12}
{'loss': 0.7879, 'grad_norm': 1.2727606296539307, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.6781144738197327, 'eval_runtime': 6.9298, 'eval_samples_per_second': 1233.941, 'eval_steps_per_second': 19.337, 'epoch': 4.49}
{'loss': 0.79, 'grad_norm': 2.2683138847351074, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.6524491310119629, 'eval_runtime': 6.2919, 'eval_samples_per_second': 1359.043, 'eval_steps_per_second': 21.297, 'epoch': 4.86}
{'loss': 0.7651, 'grad_norm': 1.3269892930984497, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.5849833488464355, 'eval_runtime': 7.1364, 'eval_samples_per_second': 1198.226, 'eval_steps_per_second': 18.777, 'epoch': 5.24}
{'loss': 0.6324, 'grad_norm': 0.8713462948799133, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.5606514811515808, 'eval_runtime': 7.0313, 'eval_samples_per_second': 1216.142, 'eval_steps_per_second': 19.058, 'epoch': 5.61}
{'loss': 0.7346, 'grad_norm': 1.2411918640136719, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.5333685874938965, 'eval_runtime': 6.9891, 'eval_samples_per_second': 1223.485, 'eval_steps_per_second': 19.173, 'epoch': 5.99}
{'loss': 0.5967, 'grad_norm': 0.7534191012382507, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.5099682807922363, 'eval_runtime': 7.2538, 'eval_samples_per_second': 1178.83, 'eval_steps_per_second': 18.473, 'epoch': 6.36}
{'loss': 0.6344, 'grad_norm': 1.3486405611038208, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.5065514445304871, 'eval_runtime': 7.1823, 'eval_samples_per_second': 1190.559, 'eval_steps_per_second': 18.657, 'epoch': 6.74}
{'loss': 0.6671, 'grad_norm': 0.8730909824371338, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.4963687062263489, 'eval_runtime': 6.8399, 'eval_samples_per_second': 1250.168, 'eval_steps_per_second': 19.591, 'epoch': 7.11}
{'loss': 0.6041, 'grad_norm': 0.7620914578437805, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.49037986993789673, 'eval_runtime': 6.6038, 'eval_samples_per_second': 1294.851, 'eval_steps_per_second': 20.291, 'epoch': 7.48}
{'loss': 0.5747, 'grad_norm': 1.1906777620315552, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.4878360331058502, 'eval_runtime': 7.0543, 'eval_samples_per_second': 1212.165, 'eval_steps_per_second': 18.995, 'epoch': 7.86}
{'loss': 0.5723, 'grad_norm': 0.6926727890968323, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.47999319434165955, 'eval_runtime': 6.8534, 'eval_samples_per_second': 1247.701, 'eval_steps_per_second': 19.552, 'epoch': 8.23}
{'loss': 0.5383, 'grad_norm': 0.9057837724685669, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.47710198163986206, 'eval_runtime': 6.5368, 'eval_samples_per_second': 1308.129, 'eval_steps_per_second': 20.499, 'epoch': 8.61}
{'loss': 0.6315, 'grad_norm': 1.002418875694275, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.47712454199790955, 'eval_runtime': 6.6651, 'eval_samples_per_second': 1282.949, 'eval_steps_per_second': 20.105, 'epoch': 8.98}
{'loss': 0.6349, 'grad_norm': 0.6909464597702026, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.47390711307525635, 'eval_runtime': 7.0402, 'eval_samples_per_second': 1214.597, 'eval_steps_per_second': 19.034, 'epoch': 9.35}
{'loss': 0.5443, 'grad_norm': 0.9955751299858093, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.47358596324920654, 'eval_runtime': 7.1171, 'eval_samples_per_second': 1201.472, 'eval_steps_per_second': 18.828, 'epoch': 9.73}
{'train_runtime': 1326.3197, 'train_samples_per_second': 515.743, 'train_steps_per_second': 8.06, 'train_loss': 248.6449270535675, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

2024-11-03 12:18:33.759342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-03 12:18:33.759643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-03 12:18:33.908721: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-03 12:18:34.198458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-03 12:18:37.029386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-3-200-2200
Traceback (most recent call last):
  File "/notebooks/SMILES_to_SPEC/notebooks/models/train_model.py", line 363, in <module>
    model = Smile2Spec(args_d)
            ^^^^^^^^^^^^^^^^^^
  File "/notebooks/SMILES_to_SPEC/notebooks/models/train_model.py", line 269, in __init__
    self.LLM = AutoModelForSequenceClassification.from_pretrained(args.get('model_name'), 
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py", line 1030, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 736, in from_dict
    setattr(config, key, value)
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 197, in __setattr__
    super().__setattr__(key, value)
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 342, in num_labels
    self.id2label = {i: f"LABEL_{i}" for i in range(num_labels)}
                                              ^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object cannot be interpreted as an integer
2024-11-03 12:19:13.943641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-03 12:19:13.943741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-03 12:19:13.945466: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-03 12:19:13.955199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-03 12:19:15.323749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-5-200-2200
Traceback (most recent call last):
  File "/notebooks/SMILES_to_SPEC/notebooks/models/train_model.py", line 363, in <module>
    model = Smile2Spec(args_d)
            ^^^^^^^^^^^^^^^^^^
  File "/notebooks/SMILES_to_SPEC/notebooks/models/train_model.py", line 269, in __init__
    self.LLM = AutoModelForSequenceClassification.from_pretrained(args.get('model_name'), 
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py", line 1030, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 736, in from_dict
    setattr(config, key, value)
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 197, in __setattr__
    super().__setattr__(key, value)
  File "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py", line 342, in num_labels
    self.id2label = {i: f"LABEL_{i}" for i in range(num_labels)}
                                              ^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object cannot be interpreted as an integer
