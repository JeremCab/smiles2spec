Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-0-200-2200
Total Params. :  4,120,825
Total Trainable Params. :  4,120,825
{'loss': 11750.315, 'grad_norm': 1216.1905517578125, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 2002.3914794921875, 'eval_runtime': 3.8745, 'eval_samples_per_second': 2206.977, 'eval_steps_per_second': 34.585, 'epoch': 0.37}
{'loss': 352.0288, 'grad_norm': 1.134652853012085, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 1.8334074020385742, 'eval_runtime': 4.5481, 'eval_samples_per_second': 1880.124, 'eval_steps_per_second': 29.463, 'epoch': 0.75}
{'loss': 1.0831, 'grad_norm': 0.1855519413948059, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 0.9084151983261108, 'eval_runtime': 4.0023, 'eval_samples_per_second': 2136.527, 'eval_steps_per_second': 33.481, 'epoch': 1.12}
{'loss': 0.9627, 'grad_norm': 0.19412700831890106, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 0.8971402645111084, 'eval_runtime': 4.4861, 'eval_samples_per_second': 1906.106, 'eval_steps_per_second': 29.87, 'epoch': 1.5}
{'loss': 1.0369, 'grad_norm': 0.17583078145980835, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.8946594595909119, 'eval_runtime': 4.2595, 'eval_samples_per_second': 2007.524, 'eval_steps_per_second': 31.459, 'epoch': 1.87}
{'loss': 0.9876, 'grad_norm': 0.19966059923171997, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.8878140449523926, 'eval_runtime': 4.1483, 'eval_samples_per_second': 2061.339, 'eval_steps_per_second': 32.303, 'epoch': 2.25}
{'loss': 0.9992, 'grad_norm': 0.24093139171600342, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.8683937191963196, 'eval_runtime': 4.518, 'eval_samples_per_second': 1892.632, 'eval_steps_per_second': 29.659, 'epoch': 2.62}
{'loss': 0.9686, 'grad_norm': 0.27393683791160583, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.8140413761138916, 'eval_runtime': 4.3901, 'eval_samples_per_second': 1947.794, 'eval_steps_per_second': 30.523, 'epoch': 2.99}
{'loss': 0.8382, 'grad_norm': 0.29516395926475525, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.7509061694145203, 'eval_runtime': 4.5584, 'eval_samples_per_second': 1875.863, 'eval_steps_per_second': 29.396, 'epoch': 3.37}
{'loss': 0.8403, 'grad_norm': 0.29663676023483276, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.6652719378471375, 'eval_runtime': 4.4232, 'eval_samples_per_second': 1933.205, 'eval_steps_per_second': 30.295, 'epoch': 3.74}
{'loss': 0.7479, 'grad_norm': 0.27154552936553955, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.6066778898239136, 'eval_runtime': 4.0449, 'eval_samples_per_second': 2114.046, 'eval_steps_per_second': 33.129, 'epoch': 4.12}
{'loss': 0.6688, 'grad_norm': 0.21082811057567596, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.5692821145057678, 'eval_runtime': 4.5799, 'eval_samples_per_second': 1867.063, 'eval_steps_per_second': 29.258, 'epoch': 4.49}
{'loss': 0.6751, 'grad_norm': 0.36419209837913513, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.5478829145431519, 'eval_runtime': 4.6146, 'eval_samples_per_second': 1853.047, 'eval_steps_per_second': 29.039, 'epoch': 4.86}
{'loss': 0.6731, 'grad_norm': 0.3104815185070038, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.5314865112304688, 'eval_runtime': 4.3876, 'eval_samples_per_second': 1948.919, 'eval_steps_per_second': 30.541, 'epoch': 5.24}
{'loss': 0.5849, 'grad_norm': 0.24608369171619415, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.5171782374382019, 'eval_runtime': 4.4284, 'eval_samples_per_second': 1930.944, 'eval_steps_per_second': 30.259, 'epoch': 5.61}
{'loss': 0.6889, 'grad_norm': 0.31240078806877136, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.5056736469268799, 'eval_runtime': 4.1737, 'eval_samples_per_second': 2048.766, 'eval_steps_per_second': 32.106, 'epoch': 5.99}
{'loss': 0.5701, 'grad_norm': 0.2899007499217987, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.49155497550964355, 'eval_runtime': 4.5889, 'eval_samples_per_second': 1863.391, 'eval_steps_per_second': 29.201, 'epoch': 6.36}
{'loss': 0.6049, 'grad_norm': 0.5962454080581665, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.48131197690963745, 'eval_runtime': 4.5464, 'eval_samples_per_second': 1880.832, 'eval_steps_per_second': 29.474, 'epoch': 6.74}
{'loss': 0.6446, 'grad_norm': 0.29811131954193115, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.47191283106803894, 'eval_runtime': 4.333, 'eval_samples_per_second': 1973.46, 'eval_steps_per_second': 30.925, 'epoch': 7.11}
{'loss': 0.5826, 'grad_norm': 0.3818349540233612, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.4632832109928131, 'eval_runtime': 4.3784, 'eval_samples_per_second': 1953.006, 'eval_steps_per_second': 30.605, 'epoch': 7.48}
{'loss': 0.5502, 'grad_norm': 0.328653484582901, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.4553700387477875, 'eval_runtime': 4.2565, 'eval_samples_per_second': 2008.94, 'eval_steps_per_second': 31.481, 'epoch': 7.86}
{'loss': 0.545, 'grad_norm': 0.3286117613315582, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.4499692916870117, 'eval_runtime': 4.2848, 'eval_samples_per_second': 1995.653, 'eval_steps_per_second': 31.273, 'epoch': 8.23}
{'loss': 0.515, 'grad_norm': 0.36753329634666443, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.4446834325790405, 'eval_runtime': 4.5755, 'eval_samples_per_second': 1868.877, 'eval_steps_per_second': 29.287, 'epoch': 8.61}
{'loss': 0.6079, 'grad_norm': 0.34891819953918457, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.4418265223503113, 'eval_runtime': 4.4267, 'eval_samples_per_second': 1931.691, 'eval_steps_per_second': 30.271, 'epoch': 8.98}
{'loss': 0.6019, 'grad_norm': 0.4041581451892853, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.43930667638778687, 'eval_runtime': 4.2849, 'eval_samples_per_second': 1995.605, 'eval_steps_per_second': 31.272, 'epoch': 9.35}
{'loss': 0.5199, 'grad_norm': 0.31571659445762634, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.43775978684425354, 'eval_runtime': 4.1237, 'eval_samples_per_second': 2073.64, 'eval_steps_per_second': 32.495, 'epoch': 9.73}
{'train_runtime': 978.1004, 'train_samples_per_second': 699.356, 'train_steps_per_second': 10.929, 'train_loss': 453.51598017436527, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-1-200-2200
Total Params. :  3,866,441
Total Trainable Params. :  3,866,441
{'loss': 12089.4163, 'grad_norm': 3087.94677734375, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 1512.65234375, 'eval_runtime': 4.0483, 'eval_samples_per_second': 2112.267, 'eval_steps_per_second': 33.101, 'epoch': 0.37}
{'loss': 201.5822, 'grad_norm': 1.958022117614746, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 1.6766574382781982, 'eval_runtime': 3.9338, 'eval_samples_per_second': 2173.706, 'eval_steps_per_second': 34.063, 'epoch': 0.75}
{'loss': 2.0149, 'grad_norm': 0.8980585336685181, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 1.0615297555923462, 'eval_runtime': 4.3038, 'eval_samples_per_second': 1986.87, 'eval_steps_per_second': 31.136, 'epoch': 1.12}
{'loss': 1.5407, 'grad_norm': 1.228735089302063, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 1.0033197402954102, 'eval_runtime': 3.9566, 'eval_samples_per_second': 2161.205, 'eval_steps_per_second': 33.868, 'epoch': 1.5}
{'loss': 1.4587, 'grad_norm': 0.7355640530586243, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.9370265007019043, 'eval_runtime': 4.3548, 'eval_samples_per_second': 1963.598, 'eval_steps_per_second': 30.771, 'epoch': 1.87}
{'loss': 1.3302, 'grad_norm': 0.7098230719566345, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.9322261810302734, 'eval_runtime': 3.8966, 'eval_samples_per_second': 2194.475, 'eval_steps_per_second': 34.389, 'epoch': 2.25}
{'loss': 1.2637, 'grad_norm': 1.2004646062850952, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.8542190194129944, 'eval_runtime': 4.4415, 'eval_samples_per_second': 1925.268, 'eval_steps_per_second': 30.17, 'epoch': 2.62}
{'loss': 1.1732, 'grad_norm': 1.219006896018982, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.7644650936126709, 'eval_runtime': 4.6989, 'eval_samples_per_second': 1819.794, 'eval_steps_per_second': 28.517, 'epoch': 2.99}
{'loss': 1.0173, 'grad_norm': 0.7304101586341858, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.6874266266822815, 'eval_runtime': 4.7016, 'eval_samples_per_second': 1818.75, 'eval_steps_per_second': 28.501, 'epoch': 3.37}
{'loss': 1.0207, 'grad_norm': 1.155593752861023, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.6344444155693054, 'eval_runtime': 4.7053, 'eval_samples_per_second': 1817.322, 'eval_steps_per_second': 28.479, 'epoch': 3.74}
{'loss': 0.9309, 'grad_norm': 0.7334393262863159, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.5961111783981323, 'eval_runtime': 4.3157, 'eval_samples_per_second': 1981.39, 'eval_steps_per_second': 31.05, 'epoch': 4.12}
{'loss': 0.8697, 'grad_norm': 0.6817572712898254, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.5934155583381653, 'eval_runtime': 4.4235, 'eval_samples_per_second': 1933.089, 'eval_steps_per_second': 30.293, 'epoch': 4.49}
{'loss': 0.8851, 'grad_norm': 0.6984915733337402, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.5840475559234619, 'eval_runtime': 4.239, 'eval_samples_per_second': 2017.226, 'eval_steps_per_second': 31.611, 'epoch': 4.86}
{'loss': 0.8751, 'grad_norm': 1.132412314414978, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.5669531226158142, 'eval_runtime': 4.2237, 'eval_samples_per_second': 2024.518, 'eval_steps_per_second': 31.726, 'epoch': 5.24}
{'loss': 0.7774, 'grad_norm': 0.9525781273841858, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.5596762895584106, 'eval_runtime': 4.4991, 'eval_samples_per_second': 1900.604, 'eval_steps_per_second': 29.784, 'epoch': 5.61}
{'loss': 0.8854, 'grad_norm': 0.6853470206260681, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.5469309687614441, 'eval_runtime': 4.1607, 'eval_samples_per_second': 2055.184, 'eval_steps_per_second': 32.206, 'epoch': 5.99}
{'loss': 0.7591, 'grad_norm': 0.6474655866622925, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.5378650426864624, 'eval_runtime': 4.1581, 'eval_samples_per_second': 2056.465, 'eval_steps_per_second': 32.226, 'epoch': 6.36}
{'loss': 0.7997, 'grad_norm': 1.0941290855407715, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.5286345481872559, 'eval_runtime': 4.2865, 'eval_samples_per_second': 1994.851, 'eval_steps_per_second': 31.261, 'epoch': 6.74}
{'loss': 0.8484, 'grad_norm': 1.8239840269088745, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.5220422744750977, 'eval_runtime': 4.1357, 'eval_samples_per_second': 2067.624, 'eval_steps_per_second': 32.401, 'epoch': 7.11}
{'loss': 0.7791, 'grad_norm': 0.7295801639556885, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.527933657169342, 'eval_runtime': 4.0628, 'eval_samples_per_second': 2104.72, 'eval_steps_per_second': 32.982, 'epoch': 7.48}
{'loss': 0.7483, 'grad_norm': 0.9387668371200562, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.5268752574920654, 'eval_runtime': 4.4127, 'eval_samples_per_second': 1937.798, 'eval_steps_per_second': 30.367, 'epoch': 7.86}
{'loss': 0.7454, 'grad_norm': 0.7046492695808411, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.5178770422935486, 'eval_runtime': 4.1444, 'eval_samples_per_second': 2063.267, 'eval_steps_per_second': 32.333, 'epoch': 8.23}
{'loss': 0.7126, 'grad_norm': 1.4233267307281494, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.5232471823692322, 'eval_runtime': 3.9766, 'eval_samples_per_second': 2150.319, 'eval_steps_per_second': 33.697, 'epoch': 8.61}
{'loss': 0.8242, 'grad_norm': 1.1110684871673584, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.5168896317481995, 'eval_runtime': 4.5216, 'eval_samples_per_second': 1891.154, 'eval_steps_per_second': 29.636, 'epoch': 8.98}
{'loss': 0.812, 'grad_norm': 0.8579075336456299, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.5160310864448547, 'eval_runtime': 4.6069, 'eval_samples_per_second': 1856.119, 'eval_steps_per_second': 29.087, 'epoch': 9.35}
{'loss': 0.7221, 'grad_norm': 0.9783796072006226, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.5158868432044983, 'eval_runtime': 4.1765, 'eval_samples_per_second': 2047.385, 'eval_steps_per_second': 32.084, 'epoch': 9.73}
{'train_runtime': 976.2334, 'train_samples_per_second': 700.693, 'train_steps_per_second': 10.95, 'train_loss': 460.81620373917696, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-3-200-2200
Total Params. :  12,752,841
Total Trainable Params. :  12,752,841
{'loss': 7743.9244, 'grad_norm': 1.9445656538009644, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 1.0099029541015625, 'eval_runtime': 4.4676, 'eval_samples_per_second': 1913.989, 'eval_steps_per_second': 29.994, 'epoch': 0.37}
{'loss': 1.4098, 'grad_norm': 1.4979139566421509, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 2.0170228481292725, 'eval_runtime': 4.5622, 'eval_samples_per_second': 1874.321, 'eval_steps_per_second': 29.372, 'epoch': 0.75}
{'loss': 1.0892, 'grad_norm': 3.014294385910034, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 8.385895729064941, 'eval_runtime': 3.8815, 'eval_samples_per_second': 2203.008, 'eval_steps_per_second': 34.523, 'epoch': 1.12}
{'loss': 1.0363, 'grad_norm': 1.9928700923919678, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 11.363754272460938, 'eval_runtime': 4.4623, 'eval_samples_per_second': 1916.282, 'eval_steps_per_second': 30.029, 'epoch': 1.5}
{'loss': 1.0107, 'grad_norm': 2.099541425704956, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 8.697815895080566, 'eval_runtime': 4.1369, 'eval_samples_per_second': 2067.022, 'eval_steps_per_second': 32.392, 'epoch': 1.87}
{'loss': 0.7904, 'grad_norm': 3.4232406616210938, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 8.960692405700684, 'eval_runtime': 4.4866, 'eval_samples_per_second': 1905.879, 'eval_steps_per_second': 29.866, 'epoch': 2.25}
{'loss': 0.746, 'grad_norm': 1.9112271070480347, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 8.135164260864258, 'eval_runtime': 3.9753, 'eval_samples_per_second': 2151.059, 'eval_steps_per_second': 33.709, 'epoch': 2.62}
{'loss': 0.7218, 'grad_norm': 2.797457218170166, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 7.9169111251831055, 'eval_runtime': 4.3322, 'eval_samples_per_second': 1973.836, 'eval_steps_per_second': 30.931, 'epoch': 2.99}
{'loss': 0.63, 'grad_norm': 2.7825570106506348, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 9.065412521362305, 'eval_runtime': 4.0679, 'eval_samples_per_second': 2102.043, 'eval_steps_per_second': 32.94, 'epoch': 3.37}
{'loss': 0.6903, 'grad_norm': 2.213975429534912, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 8.290973663330078, 'eval_runtime': 4.3433, 'eval_samples_per_second': 1968.799, 'eval_steps_per_second': 30.852, 'epoch': 3.74}
{'loss': 0.6469, 'grad_norm': 1.7872848510742188, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 7.718775749206543, 'eval_runtime': 4.1759, 'eval_samples_per_second': 2047.709, 'eval_steps_per_second': 32.089, 'epoch': 4.12}
{'loss': 0.602, 'grad_norm': 2.694385290145874, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 9.070048332214355, 'eval_runtime': 4.4517, 'eval_samples_per_second': 1920.841, 'eval_steps_per_second': 30.101, 'epoch': 4.49}
{'loss': 0.6281, 'grad_norm': 3.9632585048675537, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 6.851187705993652, 'eval_runtime': 4.5434, 'eval_samples_per_second': 1882.086, 'eval_steps_per_second': 29.494, 'epoch': 4.86}
{'loss': 0.6201, 'grad_norm': 2.511793375015259, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 6.8742570877075195, 'eval_runtime': 4.2182, 'eval_samples_per_second': 2027.183, 'eval_steps_per_second': 31.767, 'epoch': 5.24}
{'loss': 0.544, 'grad_norm': 3.2270212173461914, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 8.269601821899414, 'eval_runtime': 4.1193, 'eval_samples_per_second': 2075.822, 'eval_steps_per_second': 32.53, 'epoch': 5.61}
{'loss': 0.628, 'grad_norm': 1.7890785932540894, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 7.534902572631836, 'eval_runtime': 3.8988, 'eval_samples_per_second': 2193.242, 'eval_steps_per_second': 34.37, 'epoch': 5.99}
{'loss': 0.5268, 'grad_norm': 1.5727473497390747, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 7.561530590057373, 'eval_runtime': 4.4915, 'eval_samples_per_second': 1903.8, 'eval_steps_per_second': 29.834, 'epoch': 6.36}
{'loss': 0.5694, 'grad_norm': 1.7350419759750366, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 7.603351593017578, 'eval_runtime': 4.7465, 'eval_samples_per_second': 1801.544, 'eval_steps_per_second': 28.231, 'epoch': 6.74}
{'loss': 0.594, 'grad_norm': 2.2378859519958496, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 7.327828407287598, 'eval_runtime': 4.2373, 'eval_samples_per_second': 2018.037, 'eval_steps_per_second': 31.624, 'epoch': 7.11}
{'loss': 0.5397, 'grad_norm': 1.8266408443450928, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 6.838478088378906, 'eval_runtime': 4.2004, 'eval_samples_per_second': 2035.779, 'eval_steps_per_second': 31.902, 'epoch': 7.48}
{'loss': 0.5154, 'grad_norm': 3.625924587249756, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 8.14143180847168, 'eval_runtime': 4.1863, 'eval_samples_per_second': 2042.609, 'eval_steps_per_second': 32.009, 'epoch': 7.86}
{'loss': 0.511, 'grad_norm': 1.4509782791137695, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 6.6096014976501465, 'eval_runtime': 4.2888, 'eval_samples_per_second': 1993.81, 'eval_steps_per_second': 31.244, 'epoch': 8.23}
{'loss': 0.4842, 'grad_norm': 1.5664293766021729, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 6.779433250427246, 'eval_runtime': 4.1065, 'eval_samples_per_second': 2082.31, 'eval_steps_per_second': 32.631, 'epoch': 8.61}
{'loss': 0.5731, 'grad_norm': 1.5747205018997192, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 6.677966117858887, 'eval_runtime': 3.9539, 'eval_samples_per_second': 2162.69, 'eval_steps_per_second': 33.891, 'epoch': 8.98}
{'loss': 0.5707, 'grad_norm': 1.4617769718170166, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 6.711269855499268, 'eval_runtime': 4.0913, 'eval_samples_per_second': 2090.041, 'eval_steps_per_second': 32.752, 'epoch': 9.35}
{'loss': 0.4876, 'grad_norm': 1.9610577821731567, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 6.899311065673828, 'eval_runtime': 4.4953, 'eval_samples_per_second': 1902.194, 'eval_steps_per_second': 29.809, 'epoch': 9.73}
{'train_runtime': 990.113, 'train_samples_per_second': 690.871, 'train_steps_per_second': 10.797, 'train_loss': 290.41879015959125, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***


