Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-0-200-2200
Total Params. :  4,120,825
Total Trainable Params. :  4,120,825
{'loss': 8891.5088, 'grad_norm': 110.16952514648438, 'learning_rate': 3.738317757009346e-05, 'epoch': 0.37}
{'eval_loss': 172.97389221191406, 'eval_runtime': 4.682, 'eval_samples_per_second': 1826.341, 'eval_steps_per_second': 28.62, 'epoch': 0.37}
{'loss': 17.9087, 'grad_norm': 0.18104150891304016, 'learning_rate': 4.724532224532225e-05, 'epoch': 0.75}
{'eval_loss': 0.9252410531044006, 'eval_runtime': 4.1073, 'eval_samples_per_second': 2081.917, 'eval_steps_per_second': 32.625, 'epoch': 0.75}
{'loss': 0.96, 'grad_norm': 0.17871733009815216, 'learning_rate': 4.308731808731809e-05, 'epoch': 1.12}
{'eval_loss': 0.8986563086509705, 'eval_runtime': 4.9089, 'eval_samples_per_second': 1741.944, 'eval_steps_per_second': 27.297, 'epoch': 1.12}
{'loss': 0.9572, 'grad_norm': 0.19960139691829681, 'learning_rate': 3.8929313929313933e-05, 'epoch': 1.5}
{'eval_loss': 0.8955464363098145, 'eval_runtime': 3.9137, 'eval_samples_per_second': 2184.898, 'eval_steps_per_second': 34.239, 'epoch': 1.5}
{'loss': 1.0343, 'grad_norm': 0.3402769863605499, 'learning_rate': 3.4771309771309775e-05, 'epoch': 1.87}
{'eval_loss': 0.8903359174728394, 'eval_runtime': 4.716, 'eval_samples_per_second': 1813.183, 'eval_steps_per_second': 28.414, 'epoch': 1.87}
{'loss': 0.9844, 'grad_norm': 0.2188863605260849, 'learning_rate': 3.061330561330562e-05, 'epoch': 2.25}
{'eval_loss': 0.8837087750434875, 'eval_runtime': 4.0202, 'eval_samples_per_second': 2127.016, 'eval_steps_per_second': 33.332, 'epoch': 2.25}
{'loss': 0.9953, 'grad_norm': 0.2851971983909607, 'learning_rate': 2.6455301455301456e-05, 'epoch': 2.62}
{'eval_loss': 0.8653879761695862, 'eval_runtime': 4.2513, 'eval_samples_per_second': 2011.405, 'eval_steps_per_second': 31.52, 'epoch': 2.62}
{'loss': 0.9778, 'grad_norm': 0.2173749953508377, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.99}
{'eval_loss': 0.8378654718399048, 'eval_runtime': 4.3103, 'eval_samples_per_second': 1983.832, 'eval_steps_per_second': 31.088, 'epoch': 2.99}
{'loss': 0.8842, 'grad_norm': 0.2495156079530716, 'learning_rate': 1.813929313929314e-05, 'epoch': 3.37}
{'eval_loss': 0.7953056693077087, 'eval_runtime': 4.9821, 'eval_samples_per_second': 1716.336, 'eval_steps_per_second': 26.896, 'epoch': 3.37}
{'loss': 0.9076, 'grad_norm': 0.27044934034347534, 'learning_rate': 1.398128898128898e-05, 'epoch': 3.74}
{'eval_loss': 0.7597233057022095, 'eval_runtime': 4.5708, 'eval_samples_per_second': 1870.775, 'eval_steps_per_second': 29.316, 'epoch': 3.74}
{'loss': 0.8598, 'grad_norm': 0.24083885550498962, 'learning_rate': 9.823284823284824e-06, 'epoch': 4.12}
{'eval_loss': 0.738442063331604, 'eval_runtime': 4.095, 'eval_samples_per_second': 2088.139, 'eval_steps_per_second': 32.723, 'epoch': 4.12}
{'loss': 0.8099, 'grad_norm': 0.2068237066268921, 'learning_rate': 5.6652806652806655e-06, 'epoch': 4.49}
{'eval_loss': 0.72437983751297, 'eval_runtime': 4.4918, 'eval_samples_per_second': 1903.701, 'eval_steps_per_second': 29.832, 'epoch': 4.49}
{'loss': 0.8342, 'grad_norm': 0.29358023405075073, 'learning_rate': 1.5072765072765075e-06, 'epoch': 4.86}
{'eval_loss': 0.7170441150665283, 'eval_runtime': 4.3996, 'eval_samples_per_second': 1943.582, 'eval_steps_per_second': 30.457, 'epoch': 4.86}
{'train_runtime': 484.9723, 'train_samples_per_second': 705.236, 'train_steps_per_second': 11.021, 'train_loss': 667.5364014185735, 'epoch': 5.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-1-200-2200
Total Params. :  3,866,441
Total Trainable Params. :  3,866,441
{'loss': 8785.0331, 'grad_norm': 29.58357048034668, 'learning_rate': 3.738317757009346e-05, 'epoch': 0.37}
{'eval_loss': 3.02726149559021, 'eval_runtime': 4.5924, 'eval_samples_per_second': 1861.978, 'eval_steps_per_second': 29.178, 'epoch': 0.37}
{'loss': 2.8629, 'grad_norm': 1.0008766651153564, 'learning_rate': 4.724532224532225e-05, 'epoch': 0.75}
{'eval_loss': 1.2224618196487427, 'eval_runtime': 4.2349, 'eval_samples_per_second': 2019.156, 'eval_steps_per_second': 31.642, 'epoch': 0.75}
{'loss': 1.6341, 'grad_norm': 1.1923315525054932, 'learning_rate': 4.308731808731809e-05, 'epoch': 1.12}
{'eval_loss': 0.9664788246154785, 'eval_runtime': 4.7688, 'eval_samples_per_second': 1793.118, 'eval_steps_per_second': 28.099, 'epoch': 1.12}
{'loss': 1.4334, 'grad_norm': 1.239457607269287, 'learning_rate': 3.8929313929313933e-05, 'epoch': 1.5}
{'eval_loss': 0.9737683534622192, 'eval_runtime': 4.4078, 'eval_samples_per_second': 1939.989, 'eval_steps_per_second': 30.401, 'epoch': 1.5}
{'loss': 1.4124, 'grad_norm': 0.7627602815628052, 'learning_rate': 3.4771309771309775e-05, 'epoch': 1.87}
{'eval_loss': 0.9318177103996277, 'eval_runtime': 4.2869, 'eval_samples_per_second': 1994.686, 'eval_steps_per_second': 31.258, 'epoch': 1.87}
{'loss': 1.306, 'grad_norm': 0.8858782649040222, 'learning_rate': 3.061330561330562e-05, 'epoch': 2.25}
{'eval_loss': 0.9257141351699829, 'eval_runtime': 4.1697, 'eval_samples_per_second': 2050.749, 'eval_steps_per_second': 32.137, 'epoch': 2.25}
{'loss': 1.2612, 'grad_norm': 1.334831953048706, 'learning_rate': 2.6455301455301456e-05, 'epoch': 2.62}
{'eval_loss': 0.8774001598358154, 'eval_runtime': 4.592, 'eval_samples_per_second': 1862.171, 'eval_steps_per_second': 29.181, 'epoch': 2.62}
{'loss': 1.1981, 'grad_norm': 1.2131106853485107, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.99}
{'eval_loss': 0.8001548051834106, 'eval_runtime': 4.0243, 'eval_samples_per_second': 2124.861, 'eval_steps_per_second': 33.298, 'epoch': 2.99}
{'loss': 1.0746, 'grad_norm': 1.5988327264785767, 'learning_rate': 1.813929313929314e-05, 'epoch': 3.37}
{'eval_loss': 0.7811158299446106, 'eval_runtime': 4.784, 'eval_samples_per_second': 1787.423, 'eval_steps_per_second': 28.01, 'epoch': 3.37}
{'loss': 1.1171, 'grad_norm': 1.3064707517623901, 'learning_rate': 1.398128898128898e-05, 'epoch': 3.74}
{'eval_loss': 0.7333239912986755, 'eval_runtime': 4.5392, 'eval_samples_per_second': 1883.818, 'eval_steps_per_second': 29.521, 'epoch': 3.74}
{'loss': 1.0636, 'grad_norm': 1.0361080169677734, 'learning_rate': 9.823284823284824e-06, 'epoch': 4.12}
{'eval_loss': 0.7303005456924438, 'eval_runtime': 4.2714, 'eval_samples_per_second': 2001.928, 'eval_steps_per_second': 31.372, 'epoch': 4.12}
{'loss': 1.0129, 'grad_norm': 0.8200038075447083, 'learning_rate': 5.6652806652806655e-06, 'epoch': 4.49}
{'eval_loss': 0.7198799848556519, 'eval_runtime': 4.582, 'eval_samples_per_second': 1866.217, 'eval_steps_per_second': 29.245, 'epoch': 4.49}
{'loss': 1.0437, 'grad_norm': 0.7802779674530029, 'learning_rate': 1.5072765072765075e-06, 'epoch': 4.86}
{'eval_loss': 0.7190648317337036, 'eval_runtime': 4.5328, 'eval_samples_per_second': 1886.491, 'eval_steps_per_second': 29.563, 'epoch': 4.86}
{'train_runtime': 490.0925, 'train_samples_per_second': 697.868, 'train_steps_per_second': 10.906, 'train_loss': 658.6986234773756, 'epoch': 5.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-3-200-2200
Total Params. :  12,752,841
Total Trainable Params. :  12,752,841
{'loss': 5630.9125, 'grad_norm': 1.7955113649368286, 'learning_rate': 3.738317757009346e-05, 'epoch': 0.37}
{'eval_loss': 0.9102520942687988, 'eval_runtime': 7.7817, 'eval_samples_per_second': 1098.86, 'eval_steps_per_second': 17.22, 'epoch': 0.37}
{'loss': 1.2927, 'grad_norm': 1.8720368146896362, 'learning_rate': 4.724532224532225e-05, 'epoch': 0.75}
{'eval_loss': 6.155773162841797, 'eval_runtime': 7.7141, 'eval_samples_per_second': 1108.483, 'eval_steps_per_second': 17.371, 'epoch': 0.75}
{'loss': 1.0389, 'grad_norm': 1.8121564388275146, 'learning_rate': 4.308731808731809e-05, 'epoch': 1.12}
{'eval_loss': 10.034059524536133, 'eval_runtime': 7.9518, 'eval_samples_per_second': 1075.351, 'eval_steps_per_second': 16.851, 'epoch': 1.12}
{'loss': 0.9954, 'grad_norm': 2.624817371368408, 'learning_rate': 3.8929313929313933e-05, 'epoch': 1.5}
{'eval_loss': 8.146627426147461, 'eval_runtime': 7.9003, 'eval_samples_per_second': 1082.367, 'eval_steps_per_second': 16.961, 'epoch': 1.5}
{'loss': 0.895, 'grad_norm': 1.4658762216567993, 'learning_rate': 3.4771309771309775e-05, 'epoch': 1.87}
{'eval_loss': 7.431396484375, 'eval_runtime': 7.9722, 'eval_samples_per_second': 1072.609, 'eval_steps_per_second': 16.809, 'epoch': 1.87}
{'loss': 0.7498, 'grad_norm': 2.669039487838745, 'learning_rate': 3.061330561330562e-05, 'epoch': 2.25}
{'eval_loss': 8.211013793945312, 'eval_runtime': 7.9196, 'eval_samples_per_second': 1079.727, 'eval_steps_per_second': 16.92, 'epoch': 2.25}
{'loss': 0.7318, 'grad_norm': 2.0852866172790527, 'learning_rate': 2.6455301455301456e-05, 'epoch': 2.62}
{'eval_loss': 7.861849308013916, 'eval_runtime': 7.8041, 'eval_samples_per_second': 1095.703, 'eval_steps_per_second': 17.17, 'epoch': 2.62}
{'loss': 0.7138, 'grad_norm': 2.0746169090270996, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.99}
{'eval_loss': 9.207953453063965, 'eval_runtime': 7.9579, 'eval_samples_per_second': 1074.524, 'eval_steps_per_second': 16.839, 'epoch': 2.99}
{'loss': 0.6287, 'grad_norm': 1.395713210105896, 'learning_rate': 1.813929313929314e-05, 'epoch': 3.37}
{'eval_loss': 8.196846961975098, 'eval_runtime': 7.8302, 'eval_samples_per_second': 1092.055, 'eval_steps_per_second': 17.113, 'epoch': 3.37}
{'loss': 0.6927, 'grad_norm': 3.428237199783325, 'learning_rate': 1.398128898128898e-05, 'epoch': 3.74}
{'eval_loss': 9.067991256713867, 'eval_runtime': 7.4027, 'eval_samples_per_second': 1155.12, 'eval_steps_per_second': 18.102, 'epoch': 3.74}
{'loss': 0.6591, 'grad_norm': 1.3938753604888916, 'learning_rate': 9.823284823284824e-06, 'epoch': 4.12}
{'eval_loss': 8.75982666015625, 'eval_runtime': 7.7127, 'eval_samples_per_second': 1108.694, 'eval_steps_per_second': 17.374, 'epoch': 4.12}
{'loss': 0.6178, 'grad_norm': 2.7949445247650146, 'learning_rate': 5.6652806652806655e-06, 'epoch': 4.49}
{'eval_loss': 8.689313888549805, 'eval_runtime': 7.34, 'eval_samples_per_second': 1164.984, 'eval_steps_per_second': 18.256, 'epoch': 4.49}
{'loss': 0.6488, 'grad_norm': 1.9834328889846802, 'learning_rate': 1.5072765072765075e-06, 'epoch': 4.86}
{'eval_loss': 9.13833236694336, 'eval_runtime': 7.8076, 'eval_samples_per_second': 1095.209, 'eval_steps_per_second': 17.163, 'epoch': 4.86}
{'train_runtime': 846.8416, 'train_samples_per_second': 403.877, 'train_steps_per_second': 6.312, 'train_loss': 422.1396467960026, 'epoch': 5.0}
Predictions saved.
*** Experiment finished ***

Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    smiles
Data type: comp
Model:     DeepChem/ChemBERTa-5M-MTR
Results folder: /storage/smiles2spec_models/smiles_comp_ChemBERTa-5M-MTR_FFNN-5-200-2200
Total Params. :  22,437,241
Total Trainable Params. :  22,437,241
{'loss': 4790.3497, 'grad_norm': 3.3353843688964844, 'learning_rate': 3.738317757009346e-05, 'epoch': 0.37}
{'eval_loss': 0.9376123547554016, 'eval_runtime': 7.6299, 'eval_samples_per_second': 1120.727, 'eval_steps_per_second': 17.563, 'epoch': 0.37}
{'loss': 1.3705, 'grad_norm': 2.4255588054656982, 'learning_rate': 4.724532224532225e-05, 'epoch': 0.75}
{'eval_loss': 5.776186943054199, 'eval_runtime': 7.775, 'eval_samples_per_second': 1099.803, 'eval_steps_per_second': 17.235, 'epoch': 0.75}
{'loss': 1.1035, 'grad_norm': 5.063546657562256, 'learning_rate': 4.308731808731809e-05, 'epoch': 1.12}
{'eval_loss': 4.346245765686035, 'eval_runtime': 7.7595, 'eval_samples_per_second': 1101.997, 'eval_steps_per_second': 17.269, 'epoch': 1.12}
{'loss': 1.0594, 'grad_norm': 3.940046548843384, 'learning_rate': 3.8929313929313933e-05, 'epoch': 1.5}
{'eval_loss': 3.442091941833496, 'eval_runtime': 8.2235, 'eval_samples_per_second': 1039.82, 'eval_steps_per_second': 16.295, 'epoch': 1.5}
{'loss': 1.1072, 'grad_norm': 2.092289447784424, 'learning_rate': 3.4771309771309775e-05, 'epoch': 1.87}
{'eval_loss': 2.15439510345459, 'eval_runtime': 7.759, 'eval_samples_per_second': 1102.069, 'eval_steps_per_second': 17.27, 'epoch': 1.87}
{'loss': 1.0403, 'grad_norm': 1.1476869583129883, 'learning_rate': 3.061330561330562e-05, 'epoch': 2.25}
{'eval_loss': 0.9680231809616089, 'eval_runtime': 7.8286, 'eval_samples_per_second': 1092.278, 'eval_steps_per_second': 17.117, 'epoch': 2.25}
{'loss': 1.0361, 'grad_norm': 1.931300163269043, 'learning_rate': 2.6455301455301456e-05, 'epoch': 2.62}
{'eval_loss': 0.8945289254188538, 'eval_runtime': 7.8672, 'eval_samples_per_second': 1086.917, 'eval_steps_per_second': 17.033, 'epoch': 2.62}
{'loss': 1.0246, 'grad_norm': 3.1453659534454346, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.99}
{'eval_loss': 0.8712502717971802, 'eval_runtime': 7.9386, 'eval_samples_per_second': 1077.14, 'eval_steps_per_second': 16.88, 'epoch': 2.99}
{'loss': 0.9445, 'grad_norm': 0.8076187372207642, 'learning_rate': 1.813929313929314e-05, 'epoch': 3.37}
{'eval_loss': 0.8739534020423889, 'eval_runtime': 7.5606, 'eval_samples_per_second': 1130.991, 'eval_steps_per_second': 17.723, 'epoch': 3.37}
{'loss': 1.0144, 'grad_norm': 1.0586740970611572, 'learning_rate': 1.398128898128898e-05, 'epoch': 3.74}
{'eval_loss': 0.8710787892341614, 'eval_runtime': 7.9334, 'eval_samples_per_second': 1077.854, 'eval_steps_per_second': 16.891, 'epoch': 3.74}
{'loss': 0.9845, 'grad_norm': 1.429436206817627, 'learning_rate': 9.823284823284824e-06, 'epoch': 4.12}
{'eval_loss': 0.8705535531044006, 'eval_runtime': 8.0033, 'eval_samples_per_second': 1068.433, 'eval_steps_per_second': 16.743, 'epoch': 4.12}
{'loss': 0.9508, 'grad_norm': 2.2964682579040527, 'learning_rate': 5.6652806652806655e-06, 'epoch': 4.49}
{'eval_loss': 0.8699280023574829, 'eval_runtime': 8.2329, 'eval_samples_per_second': 1038.644, 'eval_steps_per_second': 16.276, 'epoch': 4.49}
{'loss': 0.9838, 'grad_norm': 0.5847243070602417, 'learning_rate': 1.5072765072765075e-06, 'epoch': 4.86}
{'eval_loss': 0.8739700317382812, 'eval_runtime': 7.7126, 'eval_samples_per_second': 1108.701, 'eval_steps_per_second': 17.374, 'epoch': 4.86}
{'train_runtime': 874.2486, 'train_samples_per_second': 391.216, 'train_steps_per_second': 6.114, 'train_loss': 359.46553669314835, 'epoch': 5.0}
Predictions saved.
*** Experiment finished ***
