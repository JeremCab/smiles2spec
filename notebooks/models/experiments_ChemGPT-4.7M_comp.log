Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    selfies
Data type: comp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_comp_ChemGPT-4.7M_FFNN-0-200-2200
Total Params. :  5,329,792
Total Trainable Params. :  5,329,792
{'loss': 11488.27, 'grad_norm': 3793.546630859375, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 4500.53515625, 'eval_runtime': 17.8346, 'eval_samples_per_second': 479.462, 'eval_steps_per_second': 7.513, 'epoch': 0.37}
{'loss': 1452.0092, 'grad_norm': 197.54449462890625, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 139.72775268554688, 'eval_runtime': 19.2659, 'eval_samples_per_second': 443.842, 'eval_steps_per_second': 6.955, 'epoch': 0.75}
{'loss': 25.9761, 'grad_norm': 0.40897709131240845, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 1.4520326852798462, 'eval_runtime': 17.6374, 'eval_samples_per_second': 484.821, 'eval_steps_per_second': 7.597, 'epoch': 1.12}
{'loss': 0.8767, 'grad_norm': 1.3777412176132202, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 0.732076108455658, 'eval_runtime': 17.7706, 'eval_samples_per_second': 481.189, 'eval_steps_per_second': 7.541, 'epoch': 1.5}
{'loss': 0.6352, 'grad_norm': 1.194212555885315, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.5931233763694763, 'eval_runtime': 17.791, 'eval_samples_per_second': 480.635, 'eval_steps_per_second': 7.532, 'epoch': 1.87}
{'loss': 0.714, 'grad_norm': 1.556457757949829, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.5349407196044922, 'eval_runtime': 17.6685, 'eval_samples_per_second': 483.968, 'eval_steps_per_second': 7.584, 'epoch': 2.25}
{'loss': 0.5223, 'grad_norm': 1.2366620302200317, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.49926793575286865, 'eval_runtime': 17.5541, 'eval_samples_per_second': 487.122, 'eval_steps_per_second': 7.634, 'epoch': 2.62}
{'loss': 0.5165, 'grad_norm': 1.5894672870635986, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.4619186818599701, 'eval_runtime': 17.3172, 'eval_samples_per_second': 493.786, 'eval_steps_per_second': 7.738, 'epoch': 2.99}
{'loss': 0.4997, 'grad_norm': 0.9755455255508423, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.43348240852355957, 'eval_runtime': 17.6483, 'eval_samples_per_second': 484.522, 'eval_steps_per_second': 7.593, 'epoch': 3.37}
{'loss': 0.4615, 'grad_norm': 1.500158429145813, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.415897399187088, 'eval_runtime': 17.7744, 'eval_samples_per_second': 481.085, 'eval_steps_per_second': 7.539, 'epoch': 3.74}
{'loss': 0.3931, 'grad_norm': 1.313088059425354, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.4000714123249054, 'eval_runtime': 17.9162, 'eval_samples_per_second': 477.279, 'eval_steps_per_second': 7.479, 'epoch': 4.12}
{'loss': 0.3775, 'grad_norm': 1.4416868686676025, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.3871929943561554, 'eval_runtime': 17.8491, 'eval_samples_per_second': 479.071, 'eval_steps_per_second': 7.507, 'epoch': 4.49}
{'loss': 0.462, 'grad_norm': 1.4701650142669678, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.37803980708122253, 'eval_runtime': 17.864, 'eval_samples_per_second': 478.673, 'eval_steps_per_second': 7.501, 'epoch': 4.86}
{'loss': 0.448, 'grad_norm': 1.0603150129318237, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.3658433258533478, 'eval_runtime': 17.8484, 'eval_samples_per_second': 479.09, 'eval_steps_per_second': 7.508, 'epoch': 5.24}
{'loss': 0.338, 'grad_norm': 1.5781798362731934, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.3558292090892792, 'eval_runtime': 17.9622, 'eval_samples_per_second': 476.054, 'eval_steps_per_second': 7.46, 'epoch': 5.61}
{'loss': 0.3526, 'grad_norm': 1.6086244583129883, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.3475674092769623, 'eval_runtime': 17.7746, 'eval_samples_per_second': 481.08, 'eval_steps_per_second': 7.539, 'epoch': 5.99}
{'loss': 0.3623, 'grad_norm': 1.8856229782104492, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.3415050208568573, 'eval_runtime': 17.9589, 'eval_samples_per_second': 476.142, 'eval_steps_per_second': 7.461, 'epoch': 6.36}
{'loss': 0.3734, 'grad_norm': 1.5785491466522217, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.3358749747276306, 'eval_runtime': 17.704, 'eval_samples_per_second': 482.997, 'eval_steps_per_second': 7.569, 'epoch': 6.74}
{'loss': 0.3113, 'grad_norm': 1.6452651023864746, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.33061420917510986, 'eval_runtime': 17.8714, 'eval_samples_per_second': 478.474, 'eval_steps_per_second': 7.498, 'epoch': 7.11}
{'loss': 0.3115, 'grad_norm': 1.3785760402679443, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.32699647545814514, 'eval_runtime': 17.7454, 'eval_samples_per_second': 481.872, 'eval_steps_per_second': 7.551, 'epoch': 7.48}
{'loss': 0.37, 'grad_norm': 1.1010667085647583, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.3215715289115906, 'eval_runtime': 17.7343, 'eval_samples_per_second': 482.174, 'eval_steps_per_second': 7.556, 'epoch': 7.86}
{'loss': 0.306, 'grad_norm': 1.1706286668777466, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.3191492259502411, 'eval_runtime': 17.949, 'eval_samples_per_second': 476.405, 'eval_steps_per_second': 7.466, 'epoch': 8.23}
{'loss': 0.2993, 'grad_norm': 1.5127041339874268, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.3177759349346161, 'eval_runtime': 17.5965, 'eval_samples_per_second': 485.949, 'eval_steps_per_second': 7.615, 'epoch': 8.61}
{'loss': 0.3552, 'grad_norm': 1.2340619564056396, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.3136040270328522, 'eval_runtime': 17.9061, 'eval_samples_per_second': 477.547, 'eval_steps_per_second': 7.483, 'epoch': 8.98}
{'loss': 0.3242, 'grad_norm': 1.37641179561615, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.3129926323890686, 'eval_runtime': 17.8332, 'eval_samples_per_second': 479.499, 'eval_steps_per_second': 7.514, 'epoch': 9.35}
{'loss': 0.3136, 'grad_norm': 1.2552634477615356, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.31171220541000366, 'eval_runtime': 18.002, 'eval_samples_per_second': 475.003, 'eval_steps_per_second': 7.444, 'epoch': 9.73}
{'train_runtime': 6827.7444, 'train_samples_per_second': 100.181, 'train_steps_per_second': 1.566, 'train_loss': 485.55234857172024, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    selfies
Data type: comp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_comp_ChemGPT-4.7M_FFNN-1-200-2200
Total Params. :  5,486,865
Total Trainable Params. :  5,486,865
{'loss': 12254.9538, 'grad_norm': 6369.083984375, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 4278.974609375, 'eval_runtime': 16.2691, 'eval_samples_per_second': 525.598, 'eval_steps_per_second': 8.236, 'epoch': 0.37}
{'loss': 820.0627, 'grad_norm': 3.079836368560791, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 15.892609596252441, 'eval_runtime': 15.9013, 'eval_samples_per_second': 537.754, 'eval_steps_per_second': 8.427, 'epoch': 0.75}
{'loss': 2.0765, 'grad_norm': 1.1362800598144531, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 20.28144645690918, 'eval_runtime': 15.8479, 'eval_samples_per_second': 539.568, 'eval_steps_per_second': 8.455, 'epoch': 1.12}
{'loss': 1.8731, 'grad_norm': 2.1854381561279297, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 0.9072871208190918, 'eval_runtime': 15.558, 'eval_samples_per_second': 549.622, 'eval_steps_per_second': 8.613, 'epoch': 1.5}
{'loss': 1.313, 'grad_norm': 1.357637882232666, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 0.8260484933853149, 'eval_runtime': 15.5137, 'eval_samples_per_second': 551.19, 'eval_steps_per_second': 8.638, 'epoch': 1.87}
{'loss': 1.3496, 'grad_norm': 0.88213711977005, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 0.7416226267814636, 'eval_runtime': 15.9136, 'eval_samples_per_second': 537.338, 'eval_steps_per_second': 8.42, 'epoch': 2.25}
{'loss': 1.0594, 'grad_norm': 3.348829507827759, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 0.6560722589492798, 'eval_runtime': 15.9929, 'eval_samples_per_second': 534.675, 'eval_steps_per_second': 8.379, 'epoch': 2.62}
{'loss': 1.0052, 'grad_norm': 1.640030860900879, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 0.6311004757881165, 'eval_runtime': 15.6792, 'eval_samples_per_second': 545.373, 'eval_steps_per_second': 8.546, 'epoch': 2.99}
{'loss': 0.9621, 'grad_norm': 4.469606876373291, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 0.5724693536758423, 'eval_runtime': 15.4638, 'eval_samples_per_second': 552.971, 'eval_steps_per_second': 8.665, 'epoch': 3.37}
{'loss': 0.8868, 'grad_norm': 5.070921421051025, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 0.6026423573493958, 'eval_runtime': 15.7141, 'eval_samples_per_second': 544.162, 'eval_steps_per_second': 8.527, 'epoch': 3.74}
{'loss': 0.7958, 'grad_norm': 1.858731746673584, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 0.5616653561592102, 'eval_runtime': 16.0798, 'eval_samples_per_second': 531.784, 'eval_steps_per_second': 8.333, 'epoch': 4.12}
{'loss': 0.7648, 'grad_norm': 2.4553258419036865, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 0.5251594185829163, 'eval_runtime': 16.0546, 'eval_samples_per_second': 532.618, 'eval_steps_per_second': 8.346, 'epoch': 4.49}
{'loss': 0.8513, 'grad_norm': 2.07266902923584, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 0.5059571266174316, 'eval_runtime': 15.7918, 'eval_samples_per_second': 541.484, 'eval_steps_per_second': 8.485, 'epoch': 4.86}
{'loss': 0.8333, 'grad_norm': 2.712094306945801, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 0.4913066327571869, 'eval_runtime': 15.6619, 'eval_samples_per_second': 545.975, 'eval_steps_per_second': 8.556, 'epoch': 5.24}
{'loss': 0.6881, 'grad_norm': 3.257272958755493, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 0.505078911781311, 'eval_runtime': 15.7062, 'eval_samples_per_second': 544.435, 'eval_steps_per_second': 8.532, 'epoch': 5.61}
{'loss': 0.6991, 'grad_norm': 4.3376874923706055, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 0.47408199310302734, 'eval_runtime': 15.5204, 'eval_samples_per_second': 550.951, 'eval_steps_per_second': 8.634, 'epoch': 5.99}
{'loss': 0.7084, 'grad_norm': 2.4791576862335205, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 0.4803604781627655, 'eval_runtime': 15.4316, 'eval_samples_per_second': 554.122, 'eval_steps_per_second': 8.683, 'epoch': 6.36}
{'loss': 0.7167, 'grad_norm': 4.9921441078186035, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 0.4583958089351654, 'eval_runtime': 15.5522, 'eval_samples_per_second': 549.826, 'eval_steps_per_second': 8.616, 'epoch': 6.74}
{'loss': 0.637, 'grad_norm': 3.0494675636291504, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 0.46110668778419495, 'eval_runtime': 15.4283, 'eval_samples_per_second': 554.241, 'eval_steps_per_second': 8.685, 'epoch': 7.11}
{'loss': 0.6366, 'grad_norm': 1.4983898401260376, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 0.4505302309989929, 'eval_runtime': 15.8651, 'eval_samples_per_second': 538.981, 'eval_steps_per_second': 8.446, 'epoch': 7.48}
{'loss': 0.6929, 'grad_norm': 5.260624885559082, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 0.4436021149158478, 'eval_runtime': 15.8506, 'eval_samples_per_second': 539.474, 'eval_steps_per_second': 8.454, 'epoch': 7.86}
{'loss': 0.6232, 'grad_norm': 1.6699676513671875, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 0.4460987448692322, 'eval_runtime': 15.8455, 'eval_samples_per_second': 539.649, 'eval_steps_per_second': 8.457, 'epoch': 8.23}
{'loss': 0.6135, 'grad_norm': 2.0778021812438965, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 0.45231711864471436, 'eval_runtime': 15.6296, 'eval_samples_per_second': 547.104, 'eval_steps_per_second': 8.573, 'epoch': 8.61}
{'loss': 0.6705, 'grad_norm': 2.177410364151001, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 0.4329524636268616, 'eval_runtime': 16.0762, 'eval_samples_per_second': 531.906, 'eval_steps_per_second': 8.335, 'epoch': 8.98}
{'loss': 0.6395, 'grad_norm': 1.5581837892532349, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 0.4336172640323639, 'eval_runtime': 15.568, 'eval_samples_per_second': 549.267, 'eval_steps_per_second': 8.607, 'epoch': 9.35}
{'loss': 0.6199, 'grad_norm': 1.6578871011734009, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 0.43181344866752625, 'eval_runtime': 15.8482, 'eval_samples_per_second': 539.556, 'eval_steps_per_second': 8.455, 'epoch': 9.73}
{'train_runtime': 6372.1178, 'train_samples_per_second': 107.344, 'train_steps_per_second': 1.678, 'train_loss': 490.0714857667043, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    selfies
Data type: comp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_comp_ChemGPT-4.7M_FFNN-3-200-2200
Total Params. :  14,373,265
Total Trainable Params. :  14,373,265
{'loss': 6080.1969, 'grad_norm': 1.989691972732544, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 3.060819625854492, 'eval_runtime': 15.7096, 'eval_samples_per_second': 544.316, 'eval_steps_per_second': 8.53, 'epoch': 0.37}
{'loss': 1.4218, 'grad_norm': 4.391910076141357, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 1.6966500282287598, 'eval_runtime': 15.6011, 'eval_samples_per_second': 548.101, 'eval_steps_per_second': 8.589, 'epoch': 0.75}
{'loss': 1.1566, 'grad_norm': 4.3613362312316895, 'learning_rate': 4.9319197588608256e-05, 'epoch': 1.12}
{'eval_loss': 4.296517848968506, 'eval_runtime': 15.9106, 'eval_samples_per_second': 537.44, 'eval_steps_per_second': 8.422, 'epoch': 1.12}
{'loss': 0.9276, 'grad_norm': 6.740176200866699, 'learning_rate': 4.7240411599625824e-05, 'epoch': 1.5}
{'eval_loss': 6.146524906158447, 'eval_runtime': 15.6438, 'eval_samples_per_second': 546.605, 'eval_steps_per_second': 8.566, 'epoch': 1.5}
{'loss': 0.7055, 'grad_norm': 3.3692660331726074, 'learning_rate': 4.5161625610643385e-05, 'epoch': 1.87}
{'eval_loss': 5.632598876953125, 'eval_runtime': 15.7475, 'eval_samples_per_second': 543.007, 'eval_steps_per_second': 8.509, 'epoch': 1.87}
{'loss': 0.7971, 'grad_norm': 5.3804521560668945, 'learning_rate': 4.308283962166095e-05, 'epoch': 2.25}
{'eval_loss': 4.692386150360107, 'eval_runtime': 15.4113, 'eval_samples_per_second': 554.851, 'eval_steps_per_second': 8.695, 'epoch': 2.25}
{'loss': 0.6122, 'grad_norm': 4.887331962585449, 'learning_rate': 4.100405363267852e-05, 'epoch': 2.62}
{'eval_loss': 6.620402812957764, 'eval_runtime': 15.3496, 'eval_samples_per_second': 557.083, 'eval_steps_per_second': 8.73, 'epoch': 2.62}
{'loss': 0.6037, 'grad_norm': 4.338271141052246, 'learning_rate': 3.892526764369608e-05, 'epoch': 2.99}
{'eval_loss': 5.569563388824463, 'eval_runtime': 15.4579, 'eval_samples_per_second': 553.181, 'eval_steps_per_second': 8.669, 'epoch': 2.99}
{'loss': 0.5871, 'grad_norm': 3.721958875656128, 'learning_rate': 3.684648165471365e-05, 'epoch': 3.37}
{'eval_loss': 5.876157283782959, 'eval_runtime': 15.9132, 'eval_samples_per_second': 537.354, 'eval_steps_per_second': 8.421, 'epoch': 3.37}
{'loss': 0.5547, 'grad_norm': 2.299603223800659, 'learning_rate': 3.476769566573121e-05, 'epoch': 3.74}
{'eval_loss': 4.812513828277588, 'eval_runtime': 15.6322, 'eval_samples_per_second': 547.013, 'eval_steps_per_second': 8.572, 'epoch': 3.74}
{'loss': 0.4881, 'grad_norm': 2.978623390197754, 'learning_rate': 3.268890967674878e-05, 'epoch': 4.12}
{'eval_loss': 5.354090213775635, 'eval_runtime': 15.6423, 'eval_samples_per_second': 546.658, 'eval_steps_per_second': 8.567, 'epoch': 4.12}
{'loss': 0.4687, 'grad_norm': 2.5664446353912354, 'learning_rate': 3.061012368776635e-05, 'epoch': 4.49}
{'eval_loss': 5.15289831161499, 'eval_runtime': 15.4374, 'eval_samples_per_second': 553.915, 'eval_steps_per_second': 8.68, 'epoch': 4.49}
{'loss': 0.5586, 'grad_norm': 4.09666633605957, 'learning_rate': 2.853133769878391e-05, 'epoch': 4.86}
{'eval_loss': 4.341122627258301, 'eval_runtime': 15.526, 'eval_samples_per_second': 550.755, 'eval_steps_per_second': 8.631, 'epoch': 4.86}
{'loss': 0.5263, 'grad_norm': 2.1507821083068848, 'learning_rate': 2.6452551709801478e-05, 'epoch': 5.24}
{'eval_loss': 4.8738017082214355, 'eval_runtime': 15.6327, 'eval_samples_per_second': 546.994, 'eval_steps_per_second': 8.572, 'epoch': 5.24}
{'loss': 0.4235, 'grad_norm': 1.9434545040130615, 'learning_rate': 2.4373765720819042e-05, 'epoch': 5.61}
{'eval_loss': 4.857996940612793, 'eval_runtime': 15.6773, 'eval_samples_per_second': 545.44, 'eval_steps_per_second': 8.547, 'epoch': 5.61}
{'loss': 0.4378, 'grad_norm': 1.972678303718567, 'learning_rate': 2.2294979731836607e-05, 'epoch': 5.99}
{'eval_loss': 4.4338579177856445, 'eval_runtime': 15.7317, 'eval_samples_per_second': 543.554, 'eval_steps_per_second': 8.518, 'epoch': 5.99}
{'loss': 0.4454, 'grad_norm': 2.2323367595672607, 'learning_rate': 2.0216193742854175e-05, 'epoch': 6.36}
{'eval_loss': 4.914803981781006, 'eval_runtime': 15.8904, 'eval_samples_per_second': 538.123, 'eval_steps_per_second': 8.433, 'epoch': 6.36}
{'loss': 0.4513, 'grad_norm': 2.2845990657806396, 'learning_rate': 1.813740775387174e-05, 'epoch': 6.74}
{'eval_loss': 5.1149516105651855, 'eval_runtime': 15.8804, 'eval_samples_per_second': 538.464, 'eval_steps_per_second': 8.438, 'epoch': 6.74}
{'loss': 0.3915, 'grad_norm': 3.126091718673706, 'learning_rate': 1.6058621764889305e-05, 'epoch': 7.11}
{'eval_loss': 4.579308032989502, 'eval_runtime': 15.594, 'eval_samples_per_second': 548.353, 'eval_steps_per_second': 8.593, 'epoch': 7.11}
{'loss': 0.393, 'grad_norm': 2.7556917667388916, 'learning_rate': 1.3979835775906871e-05, 'epoch': 7.48}
{'eval_loss': 4.332226276397705, 'eval_runtime': 15.6028, 'eval_samples_per_second': 548.042, 'eval_steps_per_second': 8.588, 'epoch': 7.48}
{'loss': 0.4432, 'grad_norm': 2.1013741493225098, 'learning_rate': 1.1901049786924437e-05, 'epoch': 7.86}
{'eval_loss': 4.664299488067627, 'eval_runtime': 15.4046, 'eval_samples_per_second': 555.095, 'eval_steps_per_second': 8.699, 'epoch': 7.86}
{'loss': 0.3826, 'grad_norm': 2.0518345832824707, 'learning_rate': 9.822263797942002e-06, 'epoch': 8.23}
{'eval_loss': 4.8157639503479, 'eval_runtime': 15.2803, 'eval_samples_per_second': 559.608, 'eval_steps_per_second': 8.769, 'epoch': 8.23}
{'loss': 0.3772, 'grad_norm': 3.863218069076538, 'learning_rate': 7.743477808959569e-06, 'epoch': 8.61}
{'eval_loss': 4.5975775718688965, 'eval_runtime': 15.3723, 'eval_samples_per_second': 556.262, 'eval_steps_per_second': 8.717, 'epoch': 8.61}
{'loss': 0.4298, 'grad_norm': 1.7399814128875732, 'learning_rate': 5.664691819977134e-06, 'epoch': 8.98}
{'eval_loss': 4.385927200317383, 'eval_runtime': 15.8334, 'eval_samples_per_second': 540.06, 'eval_steps_per_second': 8.463, 'epoch': 8.98}
{'loss': 0.4015, 'grad_norm': 3.125567674636841, 'learning_rate': 3.585905830994699e-06, 'epoch': 9.35}
{'eval_loss': 4.618542671203613, 'eval_runtime': 15.8988, 'eval_samples_per_second': 537.839, 'eval_steps_per_second': 8.428, 'epoch': 9.35}
{'loss': 0.3873, 'grad_norm': 1.9544141292572021, 'learning_rate': 1.5071198420122647e-06, 'epoch': 9.73}
{'eval_loss': 4.425428867340088, 'eval_runtime': 15.6897, 'eval_samples_per_second': 545.007, 'eval_steps_per_second': 8.541, 'epoch': 9.73}
{'train_runtime': 6334.7017, 'train_samples_per_second': 107.978, 'train_steps_per_second': 1.688, 'train_loss': 228.05724739167488, 'epoch': 10.0}
Predictions saved.
*** Experiment finished ***

Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
*** Start experiment ***
Inputs:    selfies
Data type: comp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_comp_ChemGPT-4.7M_FFNN-5-200-2200
Total Params. :  24,057,665
Total Trainable Params. :  24,057,665
{'loss': 5629.4306, 'grad_norm': 10.183698654174805, 'learning_rate': 1.8709073900841907e-05, 'epoch': 0.37}
{'eval_loss': 0.9736570119857788, 'eval_runtime': 28.1881, 'eval_samples_per_second': 303.355, 'eval_steps_per_second': 4.754, 'epoch': 0.37}
{'loss': 1.4937, 'grad_norm': 3.866954803466797, 'learning_rate': 3.7418147801683815e-05, 'epoch': 0.75}
{'eval_loss': 4.602302074432373, 'eval_runtime': 28.1439, 'eval_samples_per_second': 303.832, 'eval_steps_per_second': 4.761, 'epoch': 0.75}
Traceback (most recent call last):
  File "/notebooks/smiles2spec/notebooks/models/train_model.py", line 431, in <module>
    results = trainer.train()
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py", line 2241, in backward
    loss.backward(**kwargs)
  File "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
