Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/notebooks/smiles2spec/notebooks/models/train_model.py:371: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
*** Start experiment ***
Inputs:    selfies
Data type: exp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_exp_ChemGPT-4.7M_FFNN-0-200-2200
Total Params. :  5,329,792
Total Trainable Params. :  5,329,792
Traceback (most recent call last):
  File "/notebooks/smiles2spec/notebooks/models/train_model.py", line 431, in <module>
    results = trainer.train()
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 3572, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
TypeError: compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/notebooks/smiles2spec/notebooks/models/train_model.py:371: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
*** Start experiment ***
Inputs:    selfies
Data type: exp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_exp_ChemGPT-4.7M_FFNN-1-200-2200
Total Params. :  5,486,865
Total Trainable Params. :  5,486,865
Traceback (most recent call last):
  File "/notebooks/smiles2spec/notebooks/models/train_model.py", line 431, in <module>
    results = trainer.train()
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 3572, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
TypeError: compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/notebooks/smiles2spec/notebooks/models/train_model.py:371: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
*** Start experiment ***
Inputs:    selfies
Data type: exp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_exp_ChemGPT-4.7M_FFNN-3-200-2200
Total Params. :  14,373,265
Total Trainable Params. :  14,373,265
Traceback (most recent call last):
  File "/notebooks/smiles2spec/notebooks/models/train_model.py", line 431, in <module>
    results = trainer.train()
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 3572, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
TypeError: compute_loss() got an unexpected keyword argument 'num_items_in_batch'
Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at ncfrey/ChemGPT-4.7M and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/notebooks/smiles2spec/notebooks/models/train_model.py:371: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
*** Start experiment ***
Inputs:    selfies
Data type: exp
Model:     ncfrey/ChemGPT-4.7M
Results folder: /storage/smiles2spec_models/selfies_exp_ChemGPT-4.7M_FFNN-5-200-2200
Total Params. :  24,057,665
Total Trainable Params. :  24,057,665
Traceback (most recent call last):
  File "/notebooks/smiles2spec/notebooks/models/train_model.py", line 431, in <module>
    results = trainer.train()
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2122, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 2474, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/usr/local/lib/python3.9/dist-packages/transformers/trainer.py", line 3572, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
TypeError: compute_loss() got an unexpected keyword argument 'num_items_in_batch'
